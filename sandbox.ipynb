{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earthquakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Run this first\n",
    "import obspy\n",
    "import numpy as np\n",
    "from obspy.clients.fdsn.client import Client\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.pyplot import figure, show, rc\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "import plotly.express as px\n",
    "import cartopy.crs as ccrs         # to plot maps with different projections\n",
    "import cartopy.feature as cfeature # to plot coastlines, land, borders, etc.\n",
    "\n",
    "from scipy.interpolate import griddata   #interpolate irregularly spaced data onto a grid\n",
    "\n",
    "import alphashape\n",
    "from descartes import PolygonPatch\n",
    "import imageio\n",
    "\n",
    "from scipy.interpolate import griddata   #interpolate irregularly spaced data onto a grid\n",
    "\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#run this cell to define functions\n",
    "\n",
    "def quakeML_Loader(filepath): #Loads quakeML file formats as Catalog\n",
    "    # Make an obspy Catalog object from the QuakeML file.\n",
    "    return obspy.core.event.read_events(filepath)\n",
    "    #Copied from Maleen's code\n",
    "    #Not sure this 1-line function actually needs to exist\n",
    "\n",
    "def generate_summary(cat): #function that generates a summary table of data from a catalog of events for identification of best candidate events\n",
    "    resource_ids = []\n",
    "    magnitudes = []\n",
    "    pick_counts =[]\n",
    "    epi_lats = []\n",
    "    epi_longs = []\n",
    "    times = []\n",
    "    depths = []\n",
    "    for event in cat:\n",
    "        resource_ids.append(event.resource_id)\n",
    "        magnitudes.append(event.magnitudes[0].mag)\n",
    "        pick_counts.append(len(event.picks))\n",
    "        epi_lats.append(event.origins[0].latitude)\n",
    "        epi_longs.append(event.origins[0].longitude)\n",
    "        times.append(event.origins[0].time)\n",
    "        depths.append(event.origins[0].depth)\n",
    "    summary = pd.DataFrame(resource_ids, magnitudes).reset_index()\n",
    "    summary.columns = ['magnitudes','resource_ids']\n",
    "    summary['pick_counts'] = pick_counts #unsure why this needs to be done on its own but it works\n",
    "    summary['epi_lats'] = epi_lats\n",
    "    summary['epi_longs'] = epi_longs\n",
    "    summary['times'] = times\n",
    "    summary['depths'] = depths\n",
    "    #these two lines can be edited for legibility and helpfulness\n",
    "    print('Biggest earthquake was', summary.sort_values(by='magnitudes', ascending = False).get('resource_ids').iloc[0],'with magnitude',str(summary.sort_values(by='magnitudes', ascending = False).get('magnitudes').iloc[0])+'.')\n",
    "    print('Best picked earthquake was', summary.sort_values(by='pick_counts', ascending = False).get('resource_ids').iloc[0],'with',summary.sort_values(by='pick_counts', ascending = False).get('pick_counts').iloc[0],'picks.')\n",
    "    #TODO get index position(s) of most relevant quakes, but for our data set it's i = 170\n",
    "    return summary\n",
    "\n",
    "def quakeML_Reader(event):\n",
    "    epicenter = (event.origins[0].latitude, event.origins[0].longitude) #stores event epicenter lat/lon as tuple\n",
    "    magnitude = event.magnitudes[0].mag #stores event magnitude\n",
    "    birthday = event.origins[0].time #stores time at which event occurs\n",
    "    results = []\n",
    "    for arrival in event.origins[0].arrivals: #goes through arrival data and notes phase, azimuthal angle, distance from epicenter and pick_id in 2D list\n",
    "        phase = arrival.phase\n",
    "        azi = arrival.azimuth\n",
    "        dist = arrival.distance * 111 #there is a note in Maleen's code about this being in degrees\n",
    "        pick_id = arrival.pick_id\n",
    "        result = [phase, azi, dist, pick_id]\n",
    "        results.append(result)\n",
    "    arrivals = pd.DataFrame(results)\n",
    "    arrivals.columns = ['phase','azimuth','distance','pick_id'] #makes DataFrame of arrivals data\n",
    "    results = []\n",
    "    for pick in event.picks: #goes through pick data and notes time of arrival, station data and pick_id in 2D list\n",
    "        pick_id = pick.resource_id\n",
    "        time = pick.time\n",
    "        network_code = pick.waveform_id.network_code\n",
    "        station_code = pick.waveform_id.station_code\n",
    "        channel_code =pick.waveform_id.channel_code\n",
    "        result = [pick_id, time, network_code, station_code, channel_code]\n",
    "        results.append(result)\n",
    "    picks = pd.DataFrame(results)\n",
    "    picks.columns = ['pick_id','time','network_code','station_code','channel_code'] #makes DataFrame of picks data\n",
    "    picks['travel_time'] = picks['time'] - birthday\n",
    "    log = arrivals.merge(picks, left_on='pick_id', right_on='pick_id').sort_values(by='travel_time') #merges arrivals and picks data\n",
    "    log['velocity'] = log['distance'] / log['travel_time']\n",
    "    bonus = (epicenter, magnitude, birthday) #TODO integrate bonus\n",
    "\n",
    "    #adds lat lon to picks\n",
    "    #code borrowed from Zoe's explore_data.ipynb\n",
    "    sta_list = np.unique(log.station_code)\n",
    "\n",
    "    # Get all the info for those stations from IRIS\n",
    "    network = \",\".join((np.unique(log.network_code)).tolist())\n",
    "    channel = \",\".join((np.unique(log.channel_code)).tolist())\n",
    "    station = \",\".join((np.unique(log.station_code)).tolist())\n",
    "\n",
    "    starttime = np.min(log['time'])\n",
    "    endtime = np.max(log['time'])\n",
    "\n",
    "    sta_metadata = Client(\"iris\").get_stations(starttime=starttime,endtime=endtime,network=network,channel=channel,station=station,location='',level='response')\n",
    "\n",
    "    station_locs = defaultdict(dict)\n",
    "    for network in sta_metadata:\n",
    "        for station in network:\n",
    "            for chn in station:\n",
    "                sid = f\"{network.code}.{station.code}.{chn.location_code}.{chn.code[:-1]}\" + chn.start_date.strftime('%Y%j')\n",
    "                if sid in station_locs:\n",
    "                    station_locs[sid][\"component\"] += f\",{chn.code[-1]}\"\n",
    "                    station_locs[sid][\"response\"] += f\",{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                else:\n",
    "                    component = f\"{chn.code[-1]}\"\n",
    "                    response = f\"{chn.response.instrument_sensitivity.value:.2f}\"\n",
    "                    dtype = chn.response.instrument_sensitivity.input_units.lower()\n",
    "                    tmp_dict = {}\n",
    "                    tmp_dict[\"longitude\"], tmp_dict[\"latitude\"], tmp_dict[\"elevation(m)\"] = (\n",
    "                        chn.longitude,\n",
    "                        chn.latitude,\n",
    "                        chn.elevation,\n",
    "                    )\n",
    "                    tmp_dict[\"component\"], tmp_dict[\"response\"], tmp_dict[\"unit\"] = component, response, dtype\n",
    "                    tmp_dict[\"start_date\"], tmp_dict[\"end_date\"] = chn.start_date,chn.end_date\n",
    "                    tmp_dict[\"network\"], tmp_dict[\"station\"] = network.code, station.code\n",
    "                    station_locs[sid] = tmp_dict\n",
    "\n",
    "    station_locs = pd.DataFrame.from_dict(station_locs,orient='index')\n",
    "    station_locs[\"id\"] = station_locs.index\n",
    "    # Remove the date from ID\n",
    "    station_locs['id']=station_locs['id'].str.slice(stop=-7)\n",
    "    loc_log = log.merge(station_locs, left_on='station_code',right_on='station').drop(columns=['component','response','unit','start_date','end_date','id','network_code','station_code'])\n",
    "    return loc_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = quakeML_Loader('XO_2019_01.quakeml') #loads quakeML file as events Catalog\n",
    "\n",
    "#Should take about 1 minute to run on Noah's laptop, be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summary = generate_summary(cat) #summarizes events and identifies most relevant\n",
    "summary.iloc[170]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = cat[170] #biggest and most documented quake\n",
    "log = quakeML_Reader(event) #be patient, this should take like 10s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_quakes = summary[(58 < summary['epi_lats']) & (59 > summary['epi_lats']) & (-156 < summary['epi_longs']) & (-155 > summary['epi_longs'])].index[0:].tolist()\n",
    "len(close_quakes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_log = quakeML_Reader(cat[57])\n",
    "for i in close_quakes[1:]:\n",
    "    event = cat[i]\n",
    "    close_log = pd.concat([close_log,quakeML_Reader(event)])\n",
    "close_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have every pick with a latitude and longitude!  Analysis may ensue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot P-Waves azimuthal\n",
    "log = close_log\n",
    "print(log.shape[0])\n",
    "plog = pd.DataFrame()\n",
    "plog = plog.assign(azi=log[log.phase == 'P'].azimuth)\n",
    "plog = plog.assign(velocity=log[log.phase == 'P'].velocity)\n",
    "plog = plog[plog['velocity'] > 4]\n",
    "\n",
    "def azi_binner(azi):\n",
    "    if azi > 360-18/2:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.round(azi/18)*18\n",
    "azis_binned = plog.get('azi').apply(azi_binner)\n",
    "new_velocity = plog.get('velocity').apply(float)\n",
    "plog = plog.assign(v = new_velocity)\n",
    "plog = plog.assign(bins = azis_binned)\n",
    "plog = plog.groupby('bins').mean().reset_index()\n",
    "\n",
    "fig = px.line_polar(plog, r=\"v\", theta=\"bins\", line_close=True, range_r=[0,8], width =500, height =500)\n",
    "fig.update_layout(title_text='P-Wave Direction (\\N{DEGREE SIGN}) vs. Speed (km/s)', title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot P-Waves azimuthal\n",
    "log = close_log\n",
    "print(log.shape[0])\n",
    "plog = pd.DataFrame()\n",
    "plog = plog.assign(azi=log[log.phase == 'S'].azimuth)\n",
    "plog = plog.assign(velocity=log[log.phase == 'S'].velocity)\n",
    "plog = plog[plog['velocity'] > 2]\n",
    "\n",
    "def azi_binner(azi):\n",
    "    if azi > 360-18/2:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.round(azi/18)*18\n",
    "azis_binned = plog.get('azi').apply(azi_binner)\n",
    "new_velocity = plog.get('velocity').apply(float)\n",
    "plog = plog.assign(v = new_velocity)\n",
    "plog = plog.assign(bins = azis_binned)\n",
    "plog = plog.groupby('bins').mean().reset_index()\n",
    "\n",
    "fig = px.line_polar(plog, r=\"v\", theta=\"bins\", line_close=True, range_r=[0,8], width =500, height =500)\n",
    "fig.update_layout(title_text='S-Wave Direction (\\N{DEGREE SIGN}) vs. Speed (km/s)', title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot Waves azimuthal\n",
    "log = close_log\n",
    "print(log.shape[0])\n",
    "slog = pd.DataFrame()\n",
    "slog = slog.assign(azi=log[log.phase == 'S'].azimuth)\n",
    "slog = slog.assign(velocity=log[log.phase == 'S'].velocity)\n",
    "slog = slog[slog['velocity'] > 1]\n",
    "\n",
    "def azi_binner(azi):\n",
    "    if azi > 360-18/2:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.round(azi/18)*18\n",
    "azis_binned = slog.get('azi').apply(azi_binner)\n",
    "new_velocity = slog.get('velocity').apply(float)\n",
    "slog = slog.assign(v = new_velocity)\n",
    "slog = slog.assign(bins = azis_binned)\n",
    "slog = slog.groupby('bins').mean().reset_index()\n",
    "\n",
    "log = close_log\n",
    "plog = pd.DataFrame()\n",
    "plog = plog.assign(azi=log[log.phase == 'P'].azimuth)\n",
    "plog = plog.assign(velocity=log[log.phase == 'P'].velocity)\n",
    "plog = plog[plog['velocity'] > 4]\n",
    "\n",
    "def azi_binner(azi):\n",
    "    if azi > 360-18/2:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.round(azi/18)*18\n",
    "azis_binned = plog.get('azi').apply(azi_binner)\n",
    "new_velocity = plog.get('velocity').apply(float)\n",
    "plog = plog.assign(v = new_velocity)\n",
    "plog = plog.assign(bins = azis_binned)\n",
    "plog = plog.groupby('bins').mean().reset_index()\n",
    "\n",
    "flog = pd.concat([plog,slog], keys=['P','S']).reset_index()\n",
    "flog['Wave'] = flog.level_0\n",
    "fig = px.line_polar(flog, r=\"v\", theta=\"bins\", color =\"Wave\", line_close=True, range_r=[0,8], width =500, height =500)\n",
    "fig.update_layout(title_text='Wave Direction (\\N{DEGREE SIGN}) vs. Speed (km/s)', title_x=0.5)\n",
    "fig.show()\n",
    "\n",
    "#I'm not installing another package, mouseover the figure for a png download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = close_log\n",
    "plog = pd.DataFrame()\n",
    "plog = plog.assign(azi=log[log.phase == 'P'].azimuth)\n",
    "plog = plog.assign(velocity=log[log.phase == 'P'].velocity)\n",
    "plt.scatter(plog.azi,plog.velocity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes wave position GIFs\n",
    "#should take a while to run\n",
    "file_names = []\n",
    "\n",
    "Lon_Honolulu=360-155.294; Lat_Honolulu=58.320\n",
    "\n",
    "for t in range(32, 140):\n",
    "    plog = log[log['phase'] == 'P']\n",
    "    plog = plog[plog['travel_time'] < t]\n",
    "    slog = log[log['phase'] == 'S']\n",
    "    slog = slog[slog['travel_time'] < t]\n",
    "    first_pick_time = int(log[log['phase'] == 'P'].travel_time.min())\n",
    "    min_lat = log[log['phase'] == 'P'].latitude.min()\n",
    "    min_lon = log[log['phase'] == 'P'].longitude.min()\n",
    "    max_lat = log[log['phase'] == 'P'].latitude.max()\n",
    "    max_lon = log[log['phase'] == 'P'].longitude.max()\n",
    "\n",
    "    # data coordinates and values\n",
    "    xp = plog.longitude\n",
    "    yp = plog.latitude\n",
    "    zp = plog.travel_time\n",
    "    xs = slog.longitude\n",
    "    ys = slog.latitude\n",
    "    zs = slog.travel_time    \n",
    "    \n",
    "\n",
    "    # specify the desired grid to interpolate data\n",
    "    numcols, numrows = 50, 50\n",
    "    xip = np.linspace(xp.min() - 2, xp.max() + 2, numcols)\n",
    "    yip = np.linspace(yp.min() - 2, yp.max() + 2, numrows)\n",
    "    xip, yip = np.meshgrid(xip, yip)\n",
    "    pzi = griddata((xp,yp),zp,(xip,yip), method='linear')  # interpolate data onto the specified grid\n",
    "    \n",
    "    xis = np.linspace(xs.min() - 2, xs.max() + 2, numcols)\n",
    "    yis = np.linspace(ys.min() - 2, ys.max() + 2, numrows)\n",
    "    xis, yis = np.meshgrid(xis, yis)\n",
    "    szi = griddata((xs,ys),zs,(xis,yis), method='linear')  # interpolate data onto the specified grid\n",
    "\n",
    "    plt.figure(figsize=(15,5))\n",
    "    ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180)); #this specifies which projection to use\n",
    "    ax.set_extent(( 360-180,360-135, 50,70), crs=ccrs.PlateCarree())\n",
    "    ax.plot(Lon_Honolulu,Lat_Honolulu, 'r*', markersize=10, alpha=1,transform=ccrs.PlateCarree())\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    plt.title(str(plog.time.iloc[t-1]))\n",
    "    gl = ax.gridlines(draw_labels=True)\n",
    "    gl.top_labels = False\n",
    "    gl.right_labels = False\n",
    "    \n",
    "    levels = 100\n",
    "    plt.contour(xip, yip, pzi, levels, colors = 'b', alpha = 0.1,\n",
    "                 transform=ccrs.PlateCarree())\n",
    "    plt.contour(xis, yis, szi, levels, colors = 'r', alpha = 0.1,\n",
    "                 transform=ccrs.PlateCarree())\n",
    "    \n",
    "    file_name = 'stills/'+str(t-1)\n",
    "    file_names.append(file_name)\n",
    "    plt.savefig(file_name)\n",
    "    plt.close()\n",
    "\n",
    "images = []\n",
    "for filename in file_names:\n",
    "    images.append(imageio.imread(str(filename+'.png')))\n",
    "imageio.mimsave('test.gif', images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots p waves\n",
    "Lon_Honolulu=360-155.294; Lat_Honolulu=58.320\n",
    "\n",
    "plog = log[log['phase'] == 'P']\n",
    "x = plog.longitude\n",
    "y = plog.latitude\n",
    "z = plog.travel_time\n",
    "\n",
    "# specify the desired grid to interpolate data\n",
    "numcols, numrows = 100, 100\n",
    "xi = np.linspace(x.min() - 2, x.max() + 2, numcols)\n",
    "yi = np.linspace(y.min() - 1, y.max() + 1, numrows)\n",
    "xi, yi = np.meshgrid(xi, yi)\n",
    "zi = griddata((x,y),z,(xi,yi), method='linear')\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "levels = range(0, 160, 20)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180)); #this specifies which projection to use\n",
    "ax.set_extent(( 360-180,360-135, 50,70), crs=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.LAND)\n",
    "cs = plt.contour(xi, yi, zi, levels, colors = 'k',\n",
    "             transform=ccrs.PlateCarree())\n",
    "plt.clabel(cs, inline=True, fontsize=10)\n",
    "sc = ax.scatter(xi,yi, c = zi, cmap = 'hot', transform=ccrs.PlateCarree())\n",
    "ax.scatter(xi,yi, cmap = 'tab20_r',alpha = 0.4)\n",
    "#ax.scatter(x,y,5,transform=ccrs.PlateCarree())\n",
    "plt.title('P Wave Travel Time')\n",
    "ax.plot(Lon_Honolulu,Lat_Honolulu, 'r*', markersize=10, alpha=1,transform=ccrs.PlateCarree())\n",
    "plt.colorbar(sc)\n",
    "\n",
    "\n",
    "\n",
    "#plt.text(Lon_Honolulu-1,Lat_Honolulu-0.25,      # add a text label\n",
    "         #'Honolulu',transform=ccrs.PlateCarree())\n",
    "\n",
    "gl = ax.gridlines(draw_labels=True)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "file_name = 'travel_time_maps/p_wave'\n",
    "#file_names.append(file_name)\n",
    "plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plots S waves\n",
    "Lon_Honolulu=360-155.294; Lat_Honolulu=58.320\n",
    "\n",
    "plog = log[log['phase'] == 'S']\n",
    "x = plog.longitude\n",
    "y = plog.latitude\n",
    "z = plog.travel_time\n",
    "\n",
    "# specify the desired grid to interpolate data\n",
    "numcols, numrows = 100, 100\n",
    "xi = np.linspace(x.min() - 1, x.max() + 1, numcols)\n",
    "yi = np.linspace(y.min() - 1, y.max() + 1, numrows)\n",
    "xi, yi = np.meshgrid(xi, yi)\n",
    "zi = griddata((x,y),z,(xi,yi), method='linear')\n",
    "\n",
    "levels = range(0, 160, 20)\n",
    "plt.figure(figsize=(15,5))\n",
    "cs = ax.contour(xi,yi,zi)\n",
    "ax = plt.axes(projection=ccrs.PlateCarree(central_longitude=180)); #this specifies which projection to use\n",
    "ax.set_extent(( 360-180,360-135, 50,70), crs=ccrs.PlateCarree())\n",
    "ax.add_feature(cfeature.COASTLINE)\n",
    "ax.add_feature(cfeature.LAND)\n",
    "cs = plt.contour(xi, yi, zi, levels, colors = 'k',\n",
    "             transform=ccrs.PlateCarree())\n",
    "plt.clabel(cs, inline=True, fontsize=10)\n",
    "sc = ax.scatter(xi,yi, c = zi, cmap = 'hot', transform=ccrs.PlateCarree())\n",
    "ax.scatter(xi,yi, c=zi, alpha = 0.4)\n",
    "#ax.scatter(x,y,5,transform=ccrs.PlateCarree())\n",
    "plt.title('S Wave Travel Time')\n",
    "ax.plot(Lon_Honolulu,Lat_Honolulu, 'r*', markersize=10, alpha=1,transform=ccrs.PlateCarree())\n",
    "plt.colorbar(sc)\n",
    "\n",
    "\n",
    "\n",
    "#plt.text(Lon_Honolulu-1,Lat_Honolulu-0.25,      # add a text label\n",
    "         #'Honolulu',transform=ccrs.PlateCarree())\n",
    "\n",
    "gl = ax.gridlines(draw_labels=True)\n",
    "gl.top_labels = False\n",
    "gl.right_labels = False\n",
    "\n",
    "file_name = 'travel_time_maps/s_wave'\n",
    "#file_names.append(file_name)\n",
    "plt.savefig(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replicating Maleen's plot, some code is copied and modified\n",
    "p_log = log[log['phase'] == 'P']\n",
    "s_log = log[log['phase'] == 'S']\n",
    "\n",
    "fig,ax = plt.subplots(1,1,figsize=[10,10])\n",
    "ax.plot(p_log['travel_time'],p_log['distance'],'.')\n",
    "ax.plot(s_log['travel_time'],s_log['distance'],'.')\n",
    "ax.set_xlabel('Travel time (s)')\n",
    "ax.set_ylabel('Distance (km)')\n",
    "ax.legend(['P-waves','S-waves'])\n",
    "\n",
    "# P-wave fit\n",
    "m,b = np.polyfit(p_log['travel_time'].tolist(),p_log['distance'].tolist(),1)\n",
    "x_p = np.linspace(0,150)\n",
    "ax.plot(x_p,x_p*m + b,'-')\n",
    "print('P-wave slope is '+str(m)+' km/s.')\n",
    "\n",
    "# S-wave fit\n",
    "m,b = np.polyfit(s_log['travel_time'].tolist(),s_log['distance'].tolist(),1)\n",
    "x_p = np.linspace(0,150)\n",
    "ax.plot(x_p,x_p*m + b,'-')\n",
    "print('S-wave slope is '+str(m)+' km/s.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
